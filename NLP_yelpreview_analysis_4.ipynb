{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yunsun2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yunsun2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk   \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import PorterStemmer, WordNetLemmatizer \n",
    "\n",
    "import string\n",
    "import collections\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('C:/Users/Yunsun2/Desktop/Github/yelp_cleaned.csv', encoding='latin-1', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment analysis;\n",
    "def pos_score(text):\n",
    "    pos_words={'ambiance', 'ambience', 'awhile', 'delight', 'generous', 'hilarious', 'homey', 'hometown', 'home'\n",
    "               'performance', 'performer', 'recover', 'tremendous', 'fantastically', 'fantastic', 'friendly', 'extraordinarily' \n",
    "               'goodwill', 'successfully', 'amaze', 'amazing', 'amazingly', 'magic', 'favourite', 'favorite', 'fresh', \n",
    "               'great', 'love', 'best', 'delicious', 'perfect', 'awesome', 'recommend', 'excellent', 'highly', 'incredible'\n",
    "               'friendly', 'wonderful', 'perfectly', 'beautiful', 'fabulous', 'cool', 'super', 'heaven'} \n",
    "    pos_score=0\n",
    "    for i in str(text).split():\n",
    "        if i in pos_words:\n",
    "            pos_score+=1\n",
    "    return pos_score\n",
    "    \n",
    "def neg_score(text):\n",
    "    neg_words={'awkwardly', 'awkward', 'butt', 'discount', 'disappointed', 'disappoint', \n",
    "               'dismay', 'dumb', 'manner', 'sue', 'mean', 'meeting', 'monte', 'unfriendly', 'nasty', 'refuse', \n",
    "               'avoid', 'soggy', 'mistake', 'upset', 'filthy', 'sad', 'lousy', 'worst',  'unprofessional', \n",
    "               'rudely','unfortunately', 'screw', 'poorly', 'crap', 'crappy', 'tasteless', 'ignore', 'disaster',\n",
    "               'overpriced', 'wrong', 'sick', 'inedible', 'stale', 'dirty', 'disappointment', 'sorry', 'yuck', \n",
    "               'flavorless', 'mess', 'dont', 'slow', 'lack', 'wouldnt', 'disappointed', 'terrible', 'overprice', \n",
    "               'gross', 'waste', 'poor', 'leave', 'disgust', 'awful', 'mediocre', 'suck', 'bland', 'wasnt', \n",
    "               'rude', 'didnt', 'horrible', 'bad'}\n",
    "    neg_score=0\n",
    "    for i in str(text).split():\n",
    "        if i in neg_words:\n",
    "            neg_score-=1\n",
    "    return neg_score\n",
    "         \n",
    "data['pos_score']=data['textc'].apply(pos_score)\n",
    "data['neg_score']=data['textc'].apply(neg_score)\n",
    "#print(data.corr())  #sentiment now is 0.49 corr with stars; \n",
    "#if change to pos_score & neg_score, now pos is 0.346 cor, and neg_score is 0.402 cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    3526\n",
       "5    3337\n",
       "3    1461\n",
       "2     927\n",
       "1     749\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#re-value stars 4 & 5\n",
    "data['star']=data['stars'].apply(lambda x: 5 if x>=4 else x)\n",
    "data.drop('stars', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>textc</th>\n",
       "      <th>text_len</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>wife birthday breakfast excellent weather perf...</td>\n",
       "      <td>445</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2011</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no idea why bad show please everyone probably ...</td>\n",
       "      <td>545</td>\n",
       "      <td>2</td>\n",
       "      <td>-3</td>\n",
       "      <td>2011</td>\n",
       "      <td>07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>love gyro plate rice so good dig candy selection</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>love chaparral dog park very convenient surrou...</td>\n",
       "      <td>198</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>general manager scott good egg not into detail...</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>2012</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \\\n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0   \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0   \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0   \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0   \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0   \n",
       "\n",
       "                                               textc  text_len  pos_score  \\\n",
       "0  wife birthday breakfast excellent weather perf...       445         10   \n",
       "1  no idea why bad show please everyone probably ...       545          2   \n",
       "2   love gyro plate rice so good dig candy selection        48          1   \n",
       "3  love chaparral dog park very convenient surrou...       198          2   \n",
       "4  general manager scott good egg not into detail...       256          2   \n",
       "\n",
       "   neg_score  year month  \n",
       "0          0  2011    01  \n",
       "1         -3  2011    07  \n",
       "2          0  2012    06  \n",
       "3          0  2010    05  \n",
       "4         -2  2012    01  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['year']=data['date'].apply(lambda x: x.split('-')[0])\n",
    "data['month']=data['date'].apply(lambda x: x.split('-')[1])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "Z=data.drop('textc', axis=1)\n",
    "X=data['textc'] #will add other features back later on for another analysis;   \n",
    "\n",
    "rs=3746\n",
    "x_train, x_test, ztrain, ztest = train_test_split(X, Z, test_size=0.3, random_state=rs) \n",
    "\n",
    "#TfidfVectorizer\n",
    "tfidfv=TfidfVectorizer(min_df=2, ngram_range=(1, 1), stop_words='english', max_features=10000, strip_accents='unicode', \n",
    "                           norm='l2')\n",
    "\n",
    "xtrain=tfidfv.fit_transform(x_train.values.astype('U')).todense()\n",
    "xtest=tfidfv.transform(x_test.values.astype('U')).todense()\n",
    "col = ['feat_'+i for i in tfidfv.get_feature_names()]  #for TfidfVectorizer\n",
    "\n",
    "_xtrain = pd.DataFrame(xtrain, columns=col)  #feature names added\n",
    "_xtest = pd.DataFrame(xtest, columns=col)  #row index from 0 to 6999 (number of reviews);  \n",
    "\n",
    "temptrain=ztrain.loc[:, ['cool', 'useful', 'funny', 'text_len', 'pos_score', 'neg_score', 'year', 'month']].reset_index(drop=True)  #remove 'sentiment' needed;\n",
    "trainfull=pd.concat([temptrain, _xtrain], axis=1)\n",
    "\n",
    "temptest=ztest.loc[:, ['cool', 'useful', 'funny', 'text_len', 'pos_score', 'neg_score', 'year', 'month']].reset_index(drop=True) #remove 'sentiment' needed;\n",
    "testfull=pd.concat([temptest, _xtest], axis=1)\n",
    "\n",
    "ytrain=ztrain['star']\n",
    "ytest=ztest['star']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression predictions with default hyparameters for Test\n",
      "\n",
      "Accuracy\n",
      "0.72\n",
      "[[  83   31    4  111]\n",
      " [  32   52   12  173]\n",
      " [  18   31   33  395]\n",
      " [   8   10   15 1992]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.59      0.36      0.45       229\n",
      "          2       0.42      0.19      0.26       269\n",
      "          3       0.52      0.07      0.12       477\n",
      "          5       0.75      0.98      0.85      2025\n",
      "\n",
      "avg / total       0.67      0.72      0.65      3000\n",
      "\n",
      "LogisticRegressionpredictions with default hyparameters for Train\n",
      "\n",
      "Accuracy\n",
      "0.781571428571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "result=pd.DataFrame()\n",
    "rs=1928\n",
    "global result\n",
    "\n",
    "'''\n",
    "model=RandomForestRegressor(random_state=rs)\n",
    "cname='RandomForestRegressor'\n",
    "\n",
    "model=XGBClassifier(random_state=rs)\n",
    "cname='XGBoost'\n",
    "'''\n",
    "model=LogisticRegression(random_state=rs)\n",
    "cname='LogisticRegression'\n",
    "\n",
    "model.fit(trainfull, ytrain)\n",
    "pred=model.predict(testfull)\n",
    "tpred=model.predict(trainfull)\n",
    "    \n",
    "#RFRegressor has not attributes of predict_proba; \n",
    "predproba=pd.DataFrame(model.predict_proba(testfull))\n",
    "tpredproba=pd.DataFrame(model.predict_proba(trainfull))\n",
    "#merge with test and train labels; \n",
    "\n",
    "print(cname + ' predictions with default hyparameters for Test'+'\\n')\n",
    "print('Accuracy')\n",
    "print(accuracy_score(ytest, pred, normalize=True))\n",
    "print(confusion_matrix(ytest, pred))\n",
    "print(classification_report(ytest, pred))  \n",
    "\n",
    "print(cname + 'predictions with default hyparameters for Train'+'\\n')\n",
    "print('Accuracy')\n",
    "print(accuracy_score(ytrain, tpred, normalize=True))\n",
    "\n",
    "#compare bad predictions of different models; \n",
    "result=pd.concat([result, pd.DataFrame(pred)], axis=1) #to merge the predicted labels back to the dataset;\n",
    "result.rename(columns={0:str(cname + '_pred')}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Much better now! Star 2 improved due to pos_ and neg_ sentiment added.\n",
    "#After adding year and month, star 3, 5 improved, while others slightly droped. \n",
    "#If cared about stars 4 and 5, then better to group them into 1 group. Very hard to differentiate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
